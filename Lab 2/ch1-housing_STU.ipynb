{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "There is a standard housing dataset that people tend to use to get started with machine learning. \n",
    "You can download it at https://archive.ics.uci.edu/ml/machine-learning-databases/housing/. \n",
    "We will be using a slightly modified version of the dataset, which has been provided along with the code files. \n",
    "The good thing is that scikit-learn provides a function to directly load this dataset:\n",
    "\"\"\"\n",
    "#if the Bostaon dataset is longer available, switch it to other dataset.\n",
    "housing_data = datasets.load_boston()\n",
    "\n",
    "\"\"\"\n",
    "The sklearn.utils.shuffle() function shuffles arrays or \n",
    "sparse matrices in a consistent way to do random permutations of collections. \n",
    "Shuffling data reduces variance and makes sure that the patterns remain general and less overfitted.\n",
    "The random_state parameter controls how we shuffle data so that we can have reproducible results. \n",
    "\"\"\"\n",
    "X, y = shuffle(housing_data.data, housing_data.target, random_state=7)\n",
    "\n",
    "# Let's divide the data into training and testing. We'll allocate 80% for training and 20% for testing:\n",
    "num_training = int(0.8 * len(X))\n",
    "X_train, y_train = X[:num_training], y[:num_training]\n",
    "X_test, y_test = X[num_training:], y[num_training:]\n",
    "\n",
    "\"\"\"\n",
    "We are now ready to fit a decision tree regression model.  \n",
    "Let's pick a tree with a maximum depth of 4, \n",
    "which means that we are not letting the tree become arbitrarily deep:\n",
    "\"\"\"\n",
    "dt_regressor = DecisionTreeRegressor(max_depth=4)\n",
    "dt_regressor.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Fit the decision tree regression model with AdaBoost:\n",
    "The AdaBoostRegressor function has been used to compare the results and \n",
    "see how AdaBoost really boosts the performance of a decision tree regressor.\n",
    "\"\"\"\n",
    "ab_regressor = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=400, random_state=7)\n",
    "ab_regressor.fit(X_train, y_train)\n",
    "\n",
    "\"\"\"\n",
    "Let's evaluate the performance of the decision tree regressor:\n",
    "1. predict() function to predict the response variable based on the test data.\n",
    "2. calculated mean squared error and explained variance. \n",
    "3.  Mean squared error is the average of the squared difference \n",
    "between actual and predicted values across all data points in the input\n",
    "4. The explained variance is an indicator that, in the form of proportion, \n",
    "indicates how much variability of our data is explained by the model in question.\n",
    "\"\"\"\n",
    "y_pred_dt = dt_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_dt)\n",
    "evs = explained_variance_score(y_test, y_pred_dt)\n",
    "print(\"#### Decision Tree performance ####\")\n",
    "print(\"Mean squared error =\", round(mse, 2))\n",
    "print(\"Explained variance score =\", round(evs, 2))\n",
    "\n",
    "#Now, let's evaluate the performance of AdaBoost:\n",
    "\n",
    "y_pred_ab = ab_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_ab)\n",
    "evs = explained_variance_score(y_test, y_pred_ab)\n",
    "print(\"#### AdaBoost performance ####\")\n",
    "print(\"Mean squared error =\", round(mse, 2))\n",
    "print(\"Explained variance score =\", round(evs, 2))\n",
    "\n",
    "\"\"\"\n",
    "DecisionTreeRegressor builds a decision tree regressor. \n",
    "Decision trees are used to predict a response or class y, from several input variables; x1, x2,â€¦,xn.\n",
    "If y is a continuous response, it's called a regression tree,\n",
    "if y is categorical, it's called a classification tree.\n",
    "\n",
    "An AdaBoost regressor is a meta-estimator that starts by equipping a regressor \n",
    "on the actual dataset and adding additional copies of the regressor on the same dataset, \n",
    "but where the weights of instances are adjusted according to the error of the current prediction. \n",
    "\"\"\"\n",
    "\n",
    "DTFImp= dt_regressor.feature_importances_\n",
    "DTFImp= 100.0 * (DTFImp / max(DTFImp))\n",
    "index_sorted = np.flipud(np.argsort(DTFImp))\n",
    "pos = np.arange(index_sorted.shape[0]) + 0.5\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(pos, DTFImp[index_sorted], align='center')\n",
    "plt.xticks(pos, housing_data.feature_names[index_sorted])\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title(\"Decision Tree regressor\")\n",
    "plt.show()\n",
    "\n",
    "ABFImp= ab_regressor.feature_importances_\n",
    "ABFImp= 100.0 * (ABFImp / max(ABFImp))\n",
    "index_sorted = np.flipud(np.argsort(ABFImp))\n",
    "pos = np.arange(index_sorted.shape[0]) + 0.5\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(pos, ABFImp[index_sorted], align='center')\n",
    "plt.xticks(pos, housing_data.feature_names[index_sorted])\n",
    "plt.ylabel('Relative Importance')\n",
    "plt.title(\"AdaBoost regressor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
